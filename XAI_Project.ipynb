{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3i81bMQ_CU3l"
      },
      "source": [
        "This notebook provides a comprehensive analysis comparing attention mechanisms with SHAP values in transformer models. We demonstrate that attention weights, do not reliably indicate feature importance for model predictions."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We use 50 example sentences. Half positive, half negative covering simple, mixed feelings, sarcasm, negations, and even short or long cases. This gives us a diverse set of inputs to really test how attention vs. SHAP behave across lots of different ways people express sentiment."
      ],
      "metadata": {
        "id": "F8GQC1c3QzFa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_sentences = [\n",
        "    # straightforward positive\n",
        "    \"This movie was absolutely fantastic!\",\n",
        "    \"Best purchase I've ever made!\",\n",
        "\n",
        "    # straightforward negative\n",
        "    \"I hate waiting in long lines.\",\n",
        "    \"The service was terrible and disappointing.\",\n",
        "\n",
        "    # basic positive\n",
        "    \"I love how this product exceeded my expectations!\",\n",
        "    \"The battery lasts all day and the screen looks gorgeous.\",\n",
        "\n",
        "    # basic negative\n",
        "    \"Absolutely horrible experience, would not recommend.\",\n",
        "    \"The food was cold and the atmosphere was dull.\",\n",
        "\n",
        "    # mixed but net positive\n",
        "    \"The plot was great and the acting surprisingly strong.\",\n",
        "    \"Great camera performance and smooth software!\",\n",
        "\n",
        "    # mixed but net negative\n",
        "    \"The battery life sucks and the software is glitchy.\",\n",
        "    \"The sequel tried hard but bored me to tears.\",\n",
        "\n",
        "    # sarcastic positive\n",
        "    \"Wow, an upgrade that actually works—my faith in tech is restored!\",\n",
        "    \"Oh look, the train arrived early; miracles do happen.\",\n",
        "\n",
        "    # sarcastic negative\n",
        "    \"Fantastic, my flight got delayed again!\",\n",
        "    \"Oh great, another traffic jam—just what I needed!\",\n",
        "\n",
        "    # negation flavoured positive\n",
        "    \"I can't complain about this service; everything ran like clockwork.\",\n",
        "    \"Not bad at all—actually kind of amazing!\",\n",
        "\n",
        "    # negation flavoured negative\n",
        "    \"I don't think I've ever been this underwhelmed by a sequel.\",\n",
        "    \"It isn't exactly a triumph of design, to put it mildly.\",\n",
        "\n",
        "    # edge case positive\n",
        "    \"My day was made when the stranger paid for my coffee.\",\n",
        "    \"Couldn't be happier with the customer support I received.\",\n",
        "\n",
        "    # edge case negative\n",
        "    \"If frustration were currency, I'd be rich after this update.\",\n",
        "    \"Nothing like a broken promise to start the day badly.\",\n",
        "\n",
        "    # short ambiguous positive\n",
        "    \"Brilliant!\",\n",
        "    \"Sweet success.\",\n",
        "\n",
        "    # short ambiguous negative\n",
        "    \"Meh.\",\n",
        "    \"Utterly lame.\",\n",
        "\n",
        "    # long positive\n",
        "    \"The concert blew me away from start to finish.\",\n",
        "\n",
        "    # long negative\n",
        "    \"This conference was a snooze fest from keynote to close.\",\n",
        "\n",
        "    # basic positive\n",
        "    \"The vacation exceeded every expectation from start to finish.\",\n",
        "    \"Customer support resolved my issue in minutes—absolute legends.\",\n",
        "\n",
        "    # basic negative\n",
        "    \"The hotel room smelled like mildew and disappointment.\",\n",
        "    \"This update bricked my phone and erased all my photos.\",\n",
        "\n",
        "    # mixed net positive\n",
        "    \"The lecture was dense, yet the Q&A session was inspiring and insightful.\",\n",
        "    \"Shipping took ages, but the packaging and quality were superb.\",\n",
        "\n",
        "    # mixed net negative\n",
        "    \"The dessert looked stunning, though it tasted like sweet cardboard.\",\n",
        "    \"The interface feels modern, but it crashes whenever I click save.\",\n",
        "\n",
        "    # sarcastic positive\n",
        "    \"Wow, the app finally loaded without freezing; must be my lucky day!\",\n",
        "    \"Amazing, my coffee order arrived exactly as requested—miracles happen.\",\n",
        "\n",
        "    # sarcastic negative\n",
        "    \"Lovely, the printer jammed again right before the deadline—fantastic timing.\",\n",
        "    \"Great, another surprise fee on my bill—just what I needed today.\",\n",
        "\n",
        "    # negation flavoured positive\n",
        "    \"I can't say I'm unhappy with the results; they're actually impressive.\",\n",
        "    \"It's hardly possible to be more satisfied than I am right now.\",\n",
        "\n",
        "    # negation flavoured negative\n",
        "    \"I wouldn't say the performance was unforgettable—in fact, I'd rather forget it.\",\n",
        "    \"Not exactly the pinnacle of craftsmanship, is it?\",\n",
        "\n",
        "    # additional edge case positive\n",
        "    \"A stranger returned my lost wallet intact, restoring my faith in humanity.\",\n",
        "    \"The sunrise painted the sky so beautifully it took my breath away.\",\n",
        "\n",
        "    # additional edge case negative\n",
        "    \"The constant drip from the ceiling turned my bedroom into a water park overnight.\",\n",
        "    \"If boredom were lethal, that board meeting would have been a massacre.\"\n",
        "]"
      ],
      "metadata": {
        "id": "ULXlwAMoGW55"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yin4wSQWCU3n"
      },
      "source": [
        "## 1. Setup and Imports"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before running this notebook, make sure you have all the required packages installed. uncomment and run the cell below to install any missing libraries This notebook is designed for **Google Colab**.\n",
        "- NOTE: If you're running itlocally, make sure you have Python and these packages installed.\n"
      ],
      "metadata": {
        "id": "4tw74BJ3SmZj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# #uncomment to install required libraries\n",
        "# !pip install -q transformers shap matplotlib seaborn pandas"
      ],
      "metadata": {
        "id": "ymretN-JS9YX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BiBHl51WCU3n"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.patches import Patch\n",
        "import matplotlib.patches as mpatches\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "from typing import List, Tuple, Dict, Any\n",
        "import math\n",
        "\n",
        "# transformers and SHAP\n",
        "from transformers import pipeline, AutoTokenizer, AutoModelForSequenceClassification\n",
        "import shap\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# print(f\"torch version:        {torch.__version__}\")\n",
        "# print(f\"transformers version: {transformers.__version__}\")\n",
        "# print(f\"shap version:         {shap.__version__}\")]\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I1TV2hMGCU3o"
      },
      "source": [
        "## 2. Model Setup and Pipeline Initialization\n",
        "\n",
        "Next, I choose a pretrained sentiment model (DistilBERT fine-tuned on SST-2) and load it so I can get both positive and negative scores for any sentence. I used the raw model and tokenizer objects because I’ll need them to check at the attention layers. Finally, I make the whole pipeline in SHAP’s TransformersPipeline with a text masker so I can explain which words really drive the prediction."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c0OHYhcECU3o"
      },
      "outputs": [],
      "source": [
        "# sentiment analysis pipeline\n",
        "model_name = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
        "classifier = pipeline(\n",
        "    \"sentiment-analysis\",\n",
        "    model=model_name,\n",
        "    return_all_scores=True,\n",
        "    device=device)\n",
        "\n",
        "# extract the model and tokenizer\n",
        "model = classifier.model\n",
        "tokenizer = classifier.tokenizer\n",
        "\n",
        "# SHAP explainer\n",
        "shap_model = shap.models.TransformersPipeline(classifier, rescale_to_logits=True)\n",
        "explainer = shap.Explainer(shap_model, shap.maskers.Text(tokenizer))\n",
        "\n",
        "model.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YS3HFfXxCU3p"
      },
      "source": [
        "## 3. Analysis Functions\n",
        "\n",
        "I define three helpers:  \n",
        "- `extract_attention_weights` get the last layer’s CLS to token attention scores,  \n",
        "- `extract_shap_values` runs SHAP and pulls out each word’s importance,  \n",
        "- `analyze_text` bind it all together by predicting sentiment, normalizing attention, calling both extractors, and returning one result."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_attention_weights(text: str, max_length: int = 128) -> Tuple[List[str], np.ndarray]:\n",
        "    inputs = tokenizer(\n",
        "        text,\n",
        "        return_tensors=\"pt\",\n",
        "        truncation=True,\n",
        "        padding=\"max_length\",\n",
        "        max_length=max_length)\n",
        "\n",
        "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs, output_attentions=True)\n",
        "    tokens = tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0])\n",
        "    # attentions, tuple of (batch, heads, seq, seq)\n",
        "    last_attn = outputs.attentions[-1]\n",
        "    avg_heads = last_attn.mean(dim=1)\n",
        "    cls_to_tokens = avg_heads[0, 0].cpu().numpy()\n",
        "\n",
        "    return tokens, cls_to_tokens\n",
        "\n",
        "def extract_shap_values(text: str, max_evals: int = 100) -> Tuple[List[str], np.ndarray, float]:\n",
        "    sv = explainer([text], max_evals=max_evals)\n",
        "    tokens = sv.data[0]\n",
        "    values = sv.values[0][:, 1]\n",
        "    base_value = float(sv.base_values[0][1])\n",
        "    return tokens, values, base_value\n",
        "\n",
        "def analyze_text(text: str) -> Dict[str, Any]:\n",
        "    # prediction scores\n",
        "    result = classifier(text)[0]\n",
        "    pos_score = next(x[\"score\"] for x in result if x[\"label\"].upper().startswith(\"POS\"))\n",
        "    neg_score = next(x[\"score\"] for x in result if x[\"label\"].upper().startswith(\"NEG\"))\n",
        "    sentiment = \"positive\" if pos_score > neg_score else \"negative\"\n",
        "    confidence = max(pos_score, neg_score)\n",
        "\n",
        "    # attention and SHAP\n",
        "    att_tokens, att_scores = extract_attention_weights(text)\n",
        "    att_norm = (att_scores - att_scores.min()) / (att_scores.max() - att_scores.min() + 1e-8)\n",
        "    shap_tokens, shap_scores, base_val = extract_shap_values(text)\n",
        "\n",
        "    return {\n",
        "        \"text\": text,\n",
        "        \"sentiment\": sentiment,\n",
        "        \"confidence\": confidence,\n",
        "        \"pos_score\": pos_score,\n",
        "        \"neg_score\": neg_score,\n",
        "        \"attention\": {\"tokens\": att_tokens, \"scores\": att_norm},\n",
        "        \"shap\": {\"tokens\": shap_tokens, \"scores\": shap_scores, \"base_value\": base_val}}"
      ],
      "metadata": {
        "id": "1kgdmAAnFxFd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IJH5fpMxCU3q"
      },
      "source": [
        "## 4. Test Sentences for Analysis\n",
        "\n",
        "We loop through each sentence and run my `analyze_text` function to get attention maps, SHAP values, and sentiment."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# analyze all sentences\n",
        "results = []\n",
        "for sentence in test_sentences:\n",
        "    print(f\"Analyzing: {sentence}\")\n",
        "    analysis = analyze_text(sentence)\n",
        "    results.append(analysis)"
      ],
      "metadata": {
        "id": "sCADLgITGbKx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# results[0]"
      ],
      "metadata": {
        "id": "nxU29OevUuTN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vGvkDG11CU3q"
      },
      "source": [
        "## 5. Side-by-Side Token Importance Visualization\n",
        "\n",
        "- On the left we see where the model looked (normalized attention)\n",
        "- On the right we see what really drove its decision (|SHAP values|)\n",
        "- Showing positive or negative impact. This makes it easy to compare word by word how attention and real importance align.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZStE-eU6CU3q"
      },
      "outputs": [],
      "source": [
        "def plot_token_importance_comparison(result: dict, figsize=(15, 6)):\n",
        "    # barchart\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=figsize)\n",
        "\n",
        "    # atttention subplot\n",
        "    att_tokens = result['attention']['tokens']\n",
        "    att_scores = result['attention']['scores']\n",
        "\n",
        "    # filter out special/pad tokens\n",
        "    filtered = [(t, s) for t, s in zip(att_tokens, att_scores) if t not in ('[CLS]', '[SEP]', '[PAD]')]\n",
        "    tokens_att, scores_att = zip(*filtered)\n",
        "\n",
        "    ax1.barh(tokens_att, scores_att, color='steelblue')\n",
        "    ax1.set_xlabel('normalized Attention score')\n",
        "    ax1.set_title('attention weights\\n(where model looked)')\n",
        "    ax1.invert_yaxis()\n",
        "\n",
        "    # SHAP subplot\n",
        "    shap_tokens = result['shap']['tokens']\n",
        "    shap_scores = result['shap']['scores']\n",
        "    colors = ['orange' if v >= 0 else 'red' for v in shap_scores]\n",
        "\n",
        "    ax2.barh(shap_tokens, np.abs(shap_scores), color=colors)\n",
        "    ax2.set_xlabel('|SHAP Value|')\n",
        "    ax2.set_title('SHAP Importance\\n(what drove the prediction)')\n",
        "    ax2.invert_yaxis()\n",
        "\n",
        "    # legend for SHAP\n",
        "    orange_patch = mpatches.Patch(color='orange', label='+ contribution')\n",
        "    red_patch = mpatches.Patch(color='red', label='- contribution')\n",
        "    ax2.legend(handles=[orange_patch, red_patch], loc='lower right')\n",
        "\n",
        "    # title\n",
        "    fig.suptitle(\n",
        "        f'sentence: \"{result[\"text\"]}\"\\n'\n",
        "        f'prediction: {result[\"sentiment\"].capitalize()} / '\n",
        "        f'confidence: {result[\"confidence\"]:.0%}')\n",
        "\n",
        "    plt.tight_layout(rect=[0, 0, 1, 0.93])\n",
        "    return fig\n",
        "\n",
        "# checking a sentences\n",
        "for res in results[29:30:]:\n",
        "    fig = plot_token_importance_comparison(res)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w2qDVSNgCU3r"
      },
      "source": [
        "## 6. Correlation Analysis Across All Samples\n",
        "\n",
        "- Check how attention and SHAP line up across all sentences. We compute a Pearson r for each pair of token scores."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xcszVgd_ccst"
      },
      "outputs": [],
      "source": [
        "# data\n",
        "correlations = []\n",
        "for result in results:\n",
        "    att = np.array(result['attention']['scores'])\n",
        "    shap_vals = np.abs(result['shap']['scores'])\n",
        "    n = min(len(att), len(shap_vals))\n",
        "    correlations.append(np.corrcoef(att[:n], shap_vals[:n])[0, 1])\n",
        "\n",
        "# thresholds & categories\n",
        "low_thr, mod_thr = 0.3, 0.55\n",
        "categories = []\n",
        "for c in correlations:\n",
        "    if c < low_thr:\n",
        "        categories.append('low')\n",
        "    elif c < mod_thr:\n",
        "        categories.append('moderate')\n",
        "    else:\n",
        "        categories.append('high')\n",
        "\n",
        "color_map = {'low': 'red', 'moderate': 'orange', 'high': 'green'}\n",
        "colors = [color_map[cat] for cat in categories]\n",
        "\n",
        "# figure sizing\n",
        "n = len(correlations)\n",
        "width = max(12, n * 0.25)\n",
        "fig, ax = plt.subplots(figsize=(width, 6))\n",
        "\n",
        "# scatter plot of correlations\n",
        "indices = np.arange(1, n+1)\n",
        "ax.scatter(indices, correlations, c=colors, s=100, edgecolor='k', alpha=0.8)\n",
        "\n",
        "# threshold lines\n",
        "ax.axhline(low_thr, color='orange', linestyle='--', linewidth=1.5, label=f'Low ≥ {low_thr}')\n",
        "ax.axhline(mod_thr, color='green', linestyle='--', linewidth=1.5, label=f'Mod ≥ {mod_thr}')\n",
        "\n",
        "# labels & ticks\n",
        "ax.set_xlabel(\"sentence index\")\n",
        "ax.set_ylabel(\"Pearson correlation\")\n",
        "ax.set_title(\"Attention vs SHAP correlation across all sentences\")\n",
        "ax.set_xticks(indices)\n",
        "ax.set_ylim(-0.1, 1.05)\n",
        "\n",
        "# legend\n",
        "legend_handles = [\n",
        "    Patch(color='green', label='High corr (≥ 0.55)'),\n",
        "    Patch(color='orange', label='Moderate (0.3–0.5)'),\n",
        "    Patch(color='red', label='Low (< 0.3)')\n",
        "]\n",
        "line_handles = [ax.lines[0], ax.lines[1]]  # threshold lines\n",
        "ax.legend(handles=legend_handles + line_handles, loc='upper right', title=\"categories & thresholds\")\n",
        "\n",
        "# summary biox\n",
        "mean_corr = np.mean(correlations)\n",
        "median_corr = np.median(correlations)\n",
        "stats_txt = f\"Mean: {mean_corr:.2f}\\nMedian: {median_corr:.2f}\\nRange: [{min(correlations):.2f}, {max(correlations):.2f}]\"\n",
        "ax.text(0.02, 0.95, stats_txt, transform=ax.transAxes,verticalalignment='top', bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Attention scores alone are a poor proxy for real feature importance. While a some of sentences show decent alignment (green)\n",
        "- but as we can see\n",
        "- The red dots remind us that “where the model looks” is only part of the picture, and SHAP reveals a different side of its reasoning."
      ],
      "metadata": {
        "id": "cuFn9HMmXLSr"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DzMxl6-qCU3r"
      },
      "source": [
        "## 7. Token-Level Disagreement Analysis\n",
        "\n",
        "Here I made a small helper to spotlight the words where attention and SHAP really disagree. First, `analyze_disagreements` trims out special tokens, lines up attention vs. SHAP scores, and ranks by their absolute difference. Then `plot_all_disagreements` lays out the top few tokens for each sentence in a neat grid so you can spot which words pull attention one way but SHAP another.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S_nkDQEoCU3r"
      },
      "outputs": [],
      "source": [
        "def analyze_disagreements(result):\n",
        "    # identify tokens where attention and SHAP disagree most\n",
        "    att_tokens = result['attention']['tokens']\n",
        "    att_scores = np.array(result['attention']['scores'])\n",
        "    shap_scores = np.abs(result['shap']['scores'])\n",
        "\n",
        "    # trim attention to real tokens (between [CLS] and [SEP])\n",
        "    if '[CLS]' in att_tokens and '[SEP]' in att_tokens:\n",
        "        start = att_tokens.index('[CLS]') + 1\n",
        "        end = att_tokens.index('[SEP]')\n",
        "        real_tokens = att_tokens[start:end]\n",
        "        real_att = att_scores[start:end]\n",
        "    else:\n",
        "        # fallback, drop any special tokens\n",
        "        special = {'[CLS]', '[SEP]', '[PAD]'}\n",
        "        real = [(t, s) for t, s in zip(att_tokens, att_scores) if t not in special]\n",
        "        real_tokens, real_att = zip(*real)\n",
        "        real_att = np.array(real_att)\n",
        "\n",
        "    # align lengths\n",
        "    n = min(len(real_tokens), len(shap_scores))\n",
        "    df = pd.DataFrame({\n",
        "        'token': real_tokens[:n],\n",
        "        'attention': real_att[:n],\n",
        "        'shap_norm': shap_scores[:n] / (shap_scores.max() + 1e-8)})\n",
        "\n",
        "    df['disagreement'] = np.abs(df['attention'] - df['shap_norm'])\n",
        "    return df.sort_values('disagreement', ascending=False)\n",
        "\n",
        "def plot_all_disagreements(results, top_k=5, cols=4, figsize=(20, 10)):\n",
        "    # plot the top_k token-level disagreements for each result in a grid.\n",
        "\n",
        "    n = len(results)\n",
        "    rows = math.ceil(n / cols)\n",
        "    fig, axes = plt.subplots(rows, cols, figsize=figsize, squeeze=False)\n",
        "    axes_flat = axes.flatten()\n",
        "\n",
        "    for idx, result in enumerate(results):\n",
        "        ax = axes_flat[idx]\n",
        "        df = analyze_disagreements(result).head(top_k)\n",
        "        x = np.arange(len(df))\n",
        "        width = 0.35\n",
        "\n",
        "        ax.bar(x - width/2, df['attention'], width, label='Attention', color='steelblue')\n",
        "        ax.bar(x + width/2, df['shap_norm'], width, label='SHAP (norm)', color='orange')\n",
        "\n",
        "        ax.set_xticks(x)\n",
        "        ax.set_xticklabels(df['token'], rotation=45, ha='right')\n",
        "        ax.set_ylim(0, 1.2)\n",
        "        ax.set_title(f\"Sentence {idx+1}\")\n",
        "\n",
        "        # legend on first subplot\n",
        "        if idx == 0:\n",
        "            ax.legend(fontsize=8)\n",
        "\n",
        "    # turn off any unused axes\n",
        "    for j in range(idx+1, len(axes_flat)):\n",
        "        axes_flat[j].axis('off')\n",
        "\n",
        "    plt.suptitle('Top token level Disagreements between ttention and SHAP')\n",
        "    plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
        "    plt.show()\n",
        "\n",
        "# plot top 5 disagreements for first 8 sentence\n",
        "plot_all_disagreements(results[:12:], top_k=5, cols=4)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**This plot makes clear that attention and SHAP often spotlight different tokens**"
      ],
      "metadata": {
        "id": "rvH1sh7gYz1H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8. Attention vs. SHAP Scatter Plot\n",
        "\n",
        "- I append every word’s normalized attention and SHAP scores across all sentences and plot them.\n",
        "- The dot color shows how much the model attended to each token."
      ],
      "metadata": {
        "id": "L2ZPUlQ5Wlay"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-55UoGFbCU3r"
      },
      "outputs": [],
      "source": [
        "all_attention = []\n",
        "all_shap = []\n",
        "\n",
        "for res in results:\n",
        "    att = np.array(res['attention']['scores'])\n",
        "    shap_vals = np.abs(res['shap']['scores'])\n",
        "    # normalize SHAP per sentence\n",
        "    shap_norm = shap_vals / (shap_vals.max() + 1e-8)\n",
        "    # align lengths\n",
        "    n = min(len(att), len(shap_norm))\n",
        "    all_attention.extend(att[:n])\n",
        "    all_shap.extend(shap_norm[:n])\n",
        "\n",
        "# convert to arrays\n",
        "all_attention = np.array(all_attention)\n",
        "all_shap = np.array(all_shap)\n",
        "\n",
        "# plot\n",
        "fig, ax = plt.subplots(figsize=(10, 8))\n",
        "\n",
        "# scatter colored by attention score\n",
        "sc = ax.scatter(\n",
        "    all_attention,\n",
        "    all_shap,\n",
        "    c=all_attention,\n",
        "    cmap='viridis',\n",
        "    s=40,\n",
        "    edgecolor='k',\n",
        "    alpha=0.6\n",
        ")\n",
        "cbar = fig.colorbar(sc, ax=ax)\n",
        "cbar.set_label('Attention score (normalized)')\n",
        "\n",
        "# perfect correlation diagonal\n",
        "ax.plot([0, 1], [0, 1], 'r--', lw=2, label='Perfect correlation')\n",
        "\n",
        "# linear fit\n",
        "coef = np.polyfit(all_attention, all_shap, 1)\n",
        "poly = np.poly1d(coef)\n",
        "ax.plot([0, 1], poly([0, 1]), 'b-', lw=2, label=f'linear fit (slope={coef[0]:.2f})')\n",
        "\n",
        "# labels, title, grid, legend\n",
        "ax.set_xlabel('Attention score - normalized')\n",
        "ax.set_ylabel('SHAP score - normalized')\n",
        "ax.set_title('Attention vs SHAP Scores across all tokens')\n",
        "ax.legend()\n",
        "ax.grid(alpha=0.3)\n",
        "\n",
        "# overall correlation annotation\n",
        "overall_corr = np.corrcoef(all_attention, all_shap)[0, 1]\n",
        "ax.text(\n",
        "    0.05, 0.95,\n",
        "    f'Pearson r = {overall_corr:.3f}',\n",
        "    transform=ax.transAxes,\n",
        "    verticalalignment='top',\n",
        "    bbox=dict(boxstyle='round', facecolor='white', alpha=0.7))\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this scatter plot, every dot is one token from across all our sentences.\n",
        "- normalized attention score on the x-axis\n",
        "- normalized |SHAP| importance on the y-axis\n",
        "\n",
        "The color of each point (from purple to yellow) tells you how strongly the model actually looked at that word, while its vertical position shows how much that word truly impacted the prediction.\n",
        "\n",
        "Interestingly we can see how **most tokens lie near the bottom** even when the model pays a lot of attention (the yellow dots on the far right), the SHAP values often stay low. That tells us attention alone can be very misleading: just because a word lights up the attention map doesn’t mean it really mattered for the final sentiment."
      ],
      "metadata": {
        "id": "cCrr79-pY3Ut"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 9. Heatmap Visualization\n",
        "\n",
        "Now I turn every sentence’s attention and SHAP scores into heatmaps side by side. Each row is one sentence.\n",
        "- Blues show where the model paid attention, Oranges show each word’s SHAP importance (normalized per sentence), and the red-blue map highlights their difference."
      ],
      "metadata": {
        "id": "m3wF3Rq3WF5P"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b--8x48gCU3s"
      },
      "outputs": [],
      "source": [
        "def create_importance_heatmap(results, max_tokens=15):\n",
        "    # heatmaps of attention and SHAP and their difference\n",
        "\n",
        "    n_samples = len(results)\n",
        "    # pre allocate matrices\n",
        "    att_matrix = np.zeros((n_samples, max_tokens))\n",
        "    shap_matrix = np.zeros((n_samples, max_tokens))\n",
        "\n",
        "    for i, res in enumerate(results):\n",
        "        # grab and truncate\n",
        "        att = np.array(res['attention']['scores'])[:max_tokens]\n",
        "        shp = np.abs(np.array(res['shap']['scores']))[:max_tokens]\n",
        "        # fill into rows\n",
        "        att_matrix[i, :att.shape[0]] = att\n",
        "        shap_matrix[i, :shp.shape[0]] = shp\n",
        "\n",
        "    # normalize SHAP per row to [0,1]\n",
        "    row_max = shap_matrix.max(axis=1, keepdims=True) + 1e-8\n",
        "    shap_norm = shap_matrix / row_max\n",
        "\n",
        "    # difference\n",
        "    diff = att_matrix - shap_norm\n",
        "\n",
        "    # plotting\n",
        "    fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(20, 6))\n",
        "    sns.heatmap(\n",
        "        att_matrix,\n",
        "        ax=ax1,\n",
        "        cmap='Blues',\n",
        "        vmin=0,\n",
        "        vmax=1,\n",
        "        cbar_kws={'label':'attention score'})\n",
        "\n",
        "    ax1.set_title('attention scores')\n",
        "    ax1.set_xlabel('token position')\n",
        "    ax1.set_ylabel('sentence index')\n",
        "\n",
        "    sns.heatmap(\n",
        "        shap_norm,\n",
        "        ax=ax2,\n",
        "        cmap='Oranges',\n",
        "        vmin=0,\n",
        "        vmax=1,\n",
        "        cbar_kws={'label':'SHAP score - normalized'})\n",
        "\n",
        "    ax2.set_title('SHAP scores')\n",
        "    ax2.set_xlabel('Token position')\n",
        "    ax2.set_ylabel('')\n",
        "\n",
        "    # symmetric color scale around zero for difference\n",
        "    max_abs = np.max(np.abs(diff))\n",
        "    sns.heatmap(\n",
        "        diff,\n",
        "        ax=ax3,\n",
        "        cmap='RdBu_r',\n",
        "        center=0,\n",
        "        vmin=-max_abs,\n",
        "        vmax=max_abs,\n",
        "        cbar_kws={'label':'attention − SHAP'})\n",
        "\n",
        "    ax3.set_title('Difference (attention − SHAP)')\n",
        "    ax3.set_xlabel('Token position')\n",
        "    ax3.set_ylabel('')\n",
        "\n",
        "    plt.suptitle('Importance Heatmaps across sentences')\n",
        "    plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
        "    return fig\n",
        "\n",
        "fig = create_importance_heatmap(results, max_tokens=15)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this heatmaps, each row is one test sentence (I’ve limited to 15 tokens per row, padding or cutting longer ones).\n",
        "\n",
        "This side-by-side comparison makes it clear that attention alone isn’t a reliable explanation—SHAP reveals an entirely different layer of what truly influences the model’s predictions."
      ],
      "metadata": {
        "id": "bq3BzA72Y6Jo"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zq6w9hHaCU3t"
      },
      "source": [
        "## 10 Results\n",
        "Finally, I print a summary of samples analyzed, average correlation, range, and low-agreement rate."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PGw-7MhDCU3t"
      },
      "outputs": [],
      "source": [
        "# summary calculations\n",
        "mean_correlation = np.mean(correlations)\n",
        "low_thr = 0.3\n",
        "low_corr_count = sum(c < low_thr for c in correlations)\n",
        "\n",
        "# summary\n",
        "print(f\"samples analyzed:       {len(results)}\")\n",
        "print(f\"average correlation:    {mean_correlation:.3f}\")\n",
        "print(f\"correlation range:      [{min(correlations):.3f}, {max(correlations):.3f}]\")\n",
        "print(f\"low correlation samples (< {low_thr}): {low_corr_count}/{len(correlations)} \"\n",
        "      f\"({low_corr_count/len(correlations)*100:.1f}%)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Attention and SHAP capture different aspects of model behavior.**\n"
      ],
      "metadata": {
        "id": "hE-pviHAjowX"
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.0"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}